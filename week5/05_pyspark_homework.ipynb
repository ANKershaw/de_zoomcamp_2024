{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a3c696-94fa-4338-97fd-98ddc5f37675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0eccf07-7d53-4622-a706-c1a91214e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/29 12:56:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/29 12:56:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set the number of executors\n",
    "# conf.set(\"spark.executor.instances\", 3)\n",
    "# conf.set(\"spark.executor.cores\", 2)\n",
    "# conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "\n",
    "# spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1cb79-b56c-4fe3-814e-67d49ba1e707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb1489e-c158-4f81-9277-c9efc03e5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad136e2f-9c59-4e78-9c3a-2835b9b470f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1897494 fhv_tripdata_2019-10.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2019-10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390ad78e-3225-4e2d-a787-e125f9422761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e019a9ac-1a10-4d9d-9d74-0bfdbb8f3f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2019-10-01 00:23:00', dropOff_datetime='2019-10-01 00:35:00', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2019-10-01 00:11:29', dropOff_datetime='2019-10-01 00:13:22', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:11:43', dropOff_datetime='2019-10-01 00:37:20', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:56:29', dropOff_datetime='2019-10-01 00:57:47', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:23:09', dropOff_datetime='2019-10-01 00:28:27', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00014')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d63f7b-b08c-4a85-b0f0-887daaa6fd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d73b14-59aa-4ad0-8e05-19d3bb7255b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf40416-d944-4f14-bee4-12f6c850aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "types.StructField('SR_Flag', types.StringType(), True),\n",
    "types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a516e199-3ba1-4986-981e-3e0664adcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400b3d5e-53e9-4bb0-86b4-d1503c9c6932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 35), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 13, 22), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 43), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 37, 20), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 56, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 57, 47), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23, 9), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 28, 27), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 0, 48), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 7, 12), PUlocationID=129, DOlocationID=129, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 47, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 53, 25), PUlocationID=57, DOlocationID=57, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 10, 6), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 19, 50), PUlocationID=173, DOlocationID=173, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 51, 37), dropOff_datetime=datetime.datetime(2019, 10, 1, 1, 6, 14), PUlocationID=226, DOlocationID=226, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 28, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 34, 33), PUlocationID=56, DOlocationID=56, SR_Flag=None, Affiliated_base_number='B00021         ')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61783db1-46a4-4e45-86e1-f6ea7d86414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of shuffle partitions\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "df = df.repartition(6)\n",
    "df = df.coalesce(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a831aa-fd9d-4397-98b0-02e5547ad163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.parquet('fhv/2019/10', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b11de5-1a88-42c9-ae26-4d21fb6279d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81de2274-72db-4893-8030-f52323850a44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964c810a-c605-4ed5-af99-a7ffb6f2c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B03016|2019-10-03 21:44:14|2019-10-03 21:57:46|         264|         174|   NULL|                B03016|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * from trips_data\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd117a41-cdb1-4109-a1fb-24fcdc1bbe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#How many taxi trips were there on the 15th of October?\n",
    "# Consider only trips that started on the 15th of October.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(1)\n",
    "FROM \n",
    "    trips_data\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = \"2019-10-15\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5606ecd-50c2-4475-bc8f-53623db2cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|unix_timestamp(2019-10-03 21:44:14, yyyy-MM-dd HH:mm:ss)|\n",
      "+--------------------------------------------------------+\n",
      "|                                              1570153454|\n",
      "+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the length of the longest trip in the dataset in hours?\n",
    "# Figure out how to get unix timestamp\n",
    "# https://sparkbyexamples.com/apache-hive/hive-date-and-timestamp-functions-examples/\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select unix_timestamp(\"2019-10-03 21:44:14\") \n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403a809e-e6b1-4183-908c-d942623553c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|(max((unix_timestamp(dropOff_datetime, yyyy-MM-dd HH:mm:ss) - unix_timestamp(pickup_datetime, yyyy-MM-dd HH:mm:ss))) / 3600)|\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                    631152.5|\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this will give a difference in seconds between pickup and dropoff\n",
    "# divide by 3600 to get hours \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    max(\n",
    "        unix_timestamp(dropOff_datetime) - unix_timestamp(pickup_datetime)\n",
    "    ) / 3600\n",
    "FROM \n",
    "    trips_data\n",
    "WHERE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab87bc4-0bae-4e77-9694-28a38d792aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-29 13:48:47--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240229%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240229T184848Z&X-Amz-Expires=300&X-Amz-Signature=ba9632b6c8d9fb60764fac054e755f9ca59f748e8b2222cbff4dc99be2e1c806&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-29 13:48:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240229%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240229T184848Z&X-Amz-Expires=300&X-Amz-Signature=ba9632b6c8d9fb60764fac054e755f9ca59f748e8b2222cbff4dc99be2e1c806&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-02-29 13:48:48 (17.7 MB/s) - ‘taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load zone lookup into temp table \n",
    "# zone lookup: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
    "\n",
    "# download csv \n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "beadcfa5-8bd5-496d-b011-0447b427e895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read into dataframe\n",
    "zone_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "\n",
    "zone_lookup.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eaf6f067-2b95-4efd-aaa7-00a94064ccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# temp table \n",
    "zone_lookup.registerTempTable('zone_lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e774a1b5-8c73-4fae-8667-907bd6250fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:=============================>                            (3 + 3) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|count(1)|                Zone|\n",
      "+--------+--------------------+\n",
      "|       1|         Jamaica Bay|\n",
      "|       2|Governor's Island...|\n",
      "|       5| Green-Wood Cemetery|\n",
      "|       8|       Broad Channel|\n",
      "|      14|     Highbridge Park|\n",
      "|      15|        Battery Park|\n",
      "|      23|Saint Michaels Ce...|\n",
      "|      25|Breezy Point/Fort...|\n",
      "|      26|Marine Park/Floyd...|\n",
      "|      29|        Astoria Park|\n",
      "|      39|    Inwood Hill Park|\n",
      "|      47|       Willets Point|\n",
      "|      53|Forest Park/Highl...|\n",
      "|      57|  Brooklyn Navy Yard|\n",
      "|      62|        Crotona Park|\n",
      "|      77|        Country Club|\n",
      "|      89|     Freshkills Park|\n",
      "|      98|       Prospect Park|\n",
      "|     105|     Columbia Street|\n",
      "|     110|  South Williamsburg|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using the zone lookup data and the FHV October 2019 data, \n",
    "# what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "# column from trips_data : PUlocationID\n",
    "# column from zone_lookup : LocationID\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(1),\n",
    "    zone_lookup.Zone \n",
    "FROM \n",
    "    trips_data\n",
    "LEFT JOIN \n",
    "    zone_lookup\n",
    "ON\n",
    "    trips_data.PUlocationID = zone_lookup.LocationID \n",
    "GROUP BY \n",
    "    zone_lookup.Zone\n",
    "ORDER BY \n",
    "    count(1) ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ed1be-db83-4a97-b273-91a403803bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
